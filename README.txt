Compatible with python 2.7

Usage:

python decisiontree.py [number of bins] [error threshold]


I got the best reuslts from number of bins = 8 and error threshold = -0.01

PROJECT DESCRIPTION:

I used the J4.8 decision tree algorithm to build a classifier for whether an email is spam or ham. A decision tree is a tree that represents a sequential list of rules that can determine what category a given item belongs to, given the item’s features. Each node represents one feature, and is “split” into child nodes based on the possible values for that feature. For continuous features, the data is split into different ranges of values for that feature, rather than into a list of possible values. To use a tree to classify one data point, we start from the top of the tree (the “root” node), and for each node we visit we choose to visit a child of that node based on our data point’s feature value for the feature that node represents. For example, if the root node splits on the feature “domain type”, and our email comes from a .edu address, we choose the child that corresponds to the value .edu for the feature “domain type”. We continue traversing the tree like so until we reach a node with no more children, also known as a leaf node. Each leaf node has a category associated with them (in this case, spam or ham), which we then use to categorize our data point.
The J4.8 algorithm is used to construct such a decision tree based on a set of data points (the training data). The algorithm works as follows: First, we create a root node associated with all of our training data. Then, we select a feature to split the training data, and create children for the root node corresponding to possible values or ranges of possible values for the feature. Then, we recursively call the algorithm on each of these children (which are each associated with a subset of the training data), choosing a new feature to split on for each. The algorithm terminates when called on a node where all of the data associated with that node belong to the same class, in which case it becomes a leaf node assigned to that class, or when we have run out of attributes to split on, in which case the node becomes a leaf node assigned to the plurality class for the associated data points. The next important detail is how to choose which feature to split in. We choose the feature based on which maximizes information gain, which is a measure of the reduction of entropy or uncertainty of which class a randomly chosen example from a set of data belongs to. To compute this, we first find the entropy H(Y) of the data set overall, where Y is a random variable corresponding to the class we are interested in. We then find H(Y|X), which is the entropy of the data conditional on the value of feature X. Then the information gain of X is then equal to H(Y) – H(Y|X). Equations for these values given below:
After constructing the initial tree, the next step is to “prune” the tree by removing nodes that are deemed unnecessary, thus turning their parents into leaf nodes. This is a critical step because the initial decision tree is prune to overfitting, or producing results that are closely tailored to the training data but have lower generalizability when applied to other data sets. Pruning makes the set of rules represented by the tree smaller, which hopefully produces better results when applied to new data.  Pruning is generally done after the initial tree generation, because we cannot determine before the fact how beneficial splitting at a given node is. Even if the information gain is small, it is possible that the split allows for a future split on some subset of the data with a large information gain. Thus, pruning is done from the bottom-up: we look at the leaf nodes and determine whether they should be removed. The pruning method I used was error-pruning, where we prune nodes based on estimating the error rate at that node and comparing it to the weighted average (weighted by sample size) of the error estimate of the node’s children. Specifically, we prune the children if the weighted average of their error is larger than the error estimate of the parent minus some threshold value. 